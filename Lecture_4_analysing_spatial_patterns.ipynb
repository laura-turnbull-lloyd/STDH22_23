{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO0wUmbb1/Tk/ZxSTIqvlMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laura-turnbull-lloyd/STDH22_23/blob/main/Lecture_4_analysing_spatial_patterns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Today we're going to be focussing on analysing the spatial distribution of hazards and looking at event effectiveness.\n",
        "\n",
        "You'll be working with a spatial dataset consisting of polygons representing landslides that occurred during the 2008 Mw 7.9 Wenchuan earthquake in China. The data represent an area of 20 x 20 km.\n"
      ],
      "metadata": {
        "id": "qwtm067jNpVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages we'll be working with\n",
        "\n",
        "GeoPandas -- which offers a pandas-like interface to working with geodata. Think of this as your tool for basic data manipulation and transformation, much like pandas. To ensure that geopandas (and its dependencies) are correctly installed, make sure you run the package install script as detailed below (there are a few issues with installing geopandas correcty on google colab!).\n",
        "\n",
        "The other packages listed are those that you've worked with before, or packages that geopandas depends on."
      ],
      "metadata": {
        "id": "kXT86n-5I7KN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXYKhvFoETr0"
      },
      "outputs": [],
      "source": [
        "!pip install pandas fiona shapely pyproj rtree \n",
        "\n",
        "import pandas as pd\n",
        "# to make sure that pandas outputs values in an easy to read format, we can customise the output format:\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "!pip install geopandas\n",
        "import geopandas as gpd\n",
        "import shapely\n",
        "import pyproj\n",
        "import rtree\n",
        "\n",
        "import seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading in landslide data\n",
        "The data have been provided for you in a shapefile format. You need to download it from Learn Ultra and upload it to Google Colab.\n",
        "\n",
        "To read in and view the data:\n"
      ],
      "metadata": {
        "id": "R0Dcyk68EVG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the shapefile\n",
        "LS = gpd.read_file(\"landslides_clip.shp\") # this is the landslide data\n",
        "\n",
        "#Let's view the data\n",
        "LS"
      ],
      "metadata": {
        "id": "orke1TRWGi-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that there are 6604 rows in the data, which means there are 6604 individual landslide polygons."
      ],
      "metadata": {
        "id": "INZEKSSa73T7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row in the landslide dataset contains a unique identifier (in column ID), the area of the polygon in m2 (area_rob), the initials of the person who mapped it (data), and the geomometry of the data (geomoetry). These attributes are linked to each individual landslide polygon. \n",
        "\n",
        "Using the `gpd.read_file`command, you read the data into a a 'geodataframe', which is very similar to a traditional, non-spatial pandas DataFrame, but with an additional column called geometry."
      ],
      "metadata": {
        "id": "SUHh3tYPDLge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Determining the coordinate system of the data\n",
        "It's important to be aware of the coordinate system if the data you're working with. You can find it out using the ```crs``` command. It's easy to make mistakes in spatial data analysis when you aren't aware of the coordinate system of your data. This becomes even more important when you are working with more than one dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "wygZ_cxMXJem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LS.crs"
      ],
      "metadata": {
        "id": "fE0L6lIYXWM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting spatial data\n",
        "We can plot the raw spatial data using the code below. In this code, we undertake the following steps:\n",
        "1. Create a figure named f with one axis named ax by using the command plt.subplots (part of the library matplotlib, which we have imported at the top of the notebook). \n",
        "Note how the method is returning two elements and we can assign each of them to objects with different name (f and ax) by simply listing them at the front of the line, separated by commas.\n",
        "2. We plot the geographies and tell the function that we want it to draw the polygons on the axis we are passing, ax. This method returns the axis with the geographies in them, so we make sure to store it on an object with the same name, ax.\n",
        "3. We draw the entire plot by calling plt.show().\n",
        "\n",
        "For more information on matplotlib plotting conventions, see [here](https://matplotlib.org/stable/tutorials/introductory/).\n",
        "\n",
        "You can also save figures outside of this notebook, using the ```plt.savefig``` command -- see below for an example. Any figures you save will be saved in the current working directory and therefore will need to be moved if you want to access them after the session has ended.\n",
        "\n"
      ],
      "metadata": {
        "id": "2wYXposAAWSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup figure and axis\n",
        "f, ax = plt.subplots(1, figsize=(16, 8))\n",
        "# Plot layer of polygons on the axis\n",
        "LS.plot(ax=ax)\n",
        "# Remove axis frames\n",
        "#ax.set_axis_off()\n",
        "# Display\n",
        "plt.show()\n",
        "\n",
        "# Save figure to a PNG file\n",
        "plt.savefig('Wenchuan_landslides.png')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# For a very high resolution image we can add the dpi in the command, e.g. \n",
        "#plt.savefig('Wenchuan_landslides.png', dpi = 1080) # I've left this line commented, as you don't need to run it for now.\n"
      ],
      "metadata": {
        "id": "kyiHN5vhLduz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've established over the previous weeks, it's always useful to undertake exploratory data analysis to establish key characteristics about the data (hazard) that we're working with.\n",
        "\n",
        "We can use the pandas summary statistics (describe) function to be able to explore the data.\n",
        "\n",
        "The describe funciton gives the:\n",
        "> count\n",
        "\n",
        "> mean\n",
        "\n",
        "> standard deviation\n",
        "\n",
        "> minimum\n",
        "\n",
        "> 25 percentile\n",
        "\n",
        "> 50 percentile\n",
        "\n",
        "> 75 percentiles\n",
        "\n",
        "> max\n",
        "\n",
        "of all numeric columns in the dataset\n"
      ],
      "metadata": {
        "id": "OLjEMa39OjA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LS.describe()"
      ],
      "metadata": {
        "id": "N9N3ImhvD4My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What's the maximum landslide size (including correct units)?**\n",
        "\n"
      ],
      "metadata": {
        "id": "eXsB-QXuPd1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For other summary infromation that is not given by the describe funciton, we can calculate this manually. For instance, we might be interested in the total area of landslides in the study area. We can calculate this using the pandas 'series sum' function."
      ],
      "metadata": {
        "id": "ZMPDkAMgI96s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_area = pd.Series(LS['area_rob'].sum(), index=['area_rob'])\n",
        "print(total_area)"
      ],
      "metadata": {
        "id": "2opH2fx0JLgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What's the total area affected by landslides in the study area?**\n",
        "\n",
        "**What percentage of the area is affected by landslides?**\n",
        "To answer this question you will have to undertake a further calculation which you can enter below:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HsGdT4SBIsL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your calculations here in order to answer the above question:\n"
      ],
      "metadata": {
        "id": "qzrRQFugTIRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1: Point pattern analysis\n",
        "Let's explore patterns in the landslide data. First, we'll explore landslide patterns by prepresenting them as points. To do this we extract a point (the centroid) from the landslide polygons that we've just been exploring."
      ],
      "metadata": {
        "id": "Lt91m8Zm7_Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting points from the landslide polygons\n",
        "points = LS.copy()\n",
        "# change geometry \n",
        "points['geometry'] = points['geometry'].centroid\n",
        "points"
      ],
      "metadata": {
        "id": "waCUPau08KBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've already established, the first step to get a sense of what the spatial dimension of this dataset looks like is to plot it. "
      ],
      "metadata": {
        "id": "a1_zltQV8bFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1, figsize=(12, 8))\n",
        "points.plot(ax=ax) # this is the landslide data represented as points\n",
        "plt.autoscale(True)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "gDfHlU0F8aqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that this plot is very cluttered. To deal with this, we can view it as a spatial or 2-D histogram. Here, we generate a regular grid (either squared or hexagonal), count how many dots fall within each grid cell. This is attractive because it is simple and intuitive. \n",
        "\n",
        "First, let's create the regular grid."
      ],
      "metadata": {
        "id": "y8kAvRFU-f1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shapely.geometry import Polygon\n",
        "\n",
        "# total area for the grid\n",
        "xmin, ymin, xmax, ymax= LS.total_bounds # this function extracts the bounding coordinates of the landslide data\n",
        "width = 500\n",
        "height = 500\n",
        "# projection of the grid - this sets the crs to be the same as the data we area already working with\n",
        "crs = LS.crs\n",
        "rows = int(np.ceil((ymax-ymin) /  height))\n",
        "cols = int(np.ceil((xmax-xmin) / width))\n",
        "XleftOrigin = xmin\n",
        "XrightOrigin = xmin + width\n",
        "YtopOrigin = ymax\n",
        "YbottomOrigin = ymax- height\n",
        "polygons = []\n",
        "for i in range(cols):\n",
        "   Ytop = YtopOrigin\n",
        "   Ybottom =YbottomOrigin\n",
        "   for j in range(rows):\n",
        "       polygons.append(Polygon([(XleftOrigin, Ytop), (XrightOrigin, Ytop), (XrightOrigin, Ybottom), (XleftOrigin, Ybottom)])) \n",
        "       Ytop = Ytop - height\n",
        "       Ybottom = Ybottom - height\n",
        "   XleftOrigin = XleftOrigin + width\n",
        "   XrightOrigin = XrightOrigin + width\n",
        "\n",
        "grid = gpd.GeoDataFrame({'geometry':polygons}, crs=crs)\n",
        "grid"
      ],
      "metadata": {
        "id": "sQTCgLlkWRcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll create a new object called `landslide_count` and store the result of a spatial join between landslide points and the grid. "
      ],
      "metadata": {
        "id": "XnQwgP8F8PMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "landslide_count = gpd.sjoin(grid, points, how='left', predicate ='contains')\n",
        "landslide_count"
      ],
      "metadata": {
        "id": "2dvLvfiE8om9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each landslide carries the ID of its corresponding grid cell (on the far left of the above table). Let's explicitly add this grid cell ID to the geodataframe:"
      ],
      "metadata": {
        "id": "KlkdxOdK9VRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "landslide_count['grid_index'] = landslide_count.index \n",
        "landslide_count"
      ],
      "metadata": {
        "id": "SCs4d7kL9lgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to count the number of landslides in each grid cell. We can do this using the dissolve tool (which simplifies data), counting the number of landslides within each grid cell."
      ],
      "metadata": {
        "id": "SQBIit1qPTVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "landslide_count_dissolve = landslide_count.dissolve(by='grid_index', aggfunc='count') \n",
        "landslide_count_dissolve"
      ],
      "metadata": {
        "id": "_e2dNicLPjTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the table above, you can see that for the attributes that were associated with the landslide point data, you now have the 'count' of how many landslides there were in each grid cell.\n",
        "\n",
        "So, for example, for grid cell with an index of 3, there were 7 landslide points within that cell. Whilst for grid cell with an index of 0, there were 0 landslides. \n",
        "\n",
        "We can now plot out the resulting landslide point density map."
      ],
      "metadata": {
        "id": "d3Mc7GN8QjZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1, figsize=(12, 8))\n",
        "landslide_count_dissolve.plot(ax=ax, column='index_right', cmap='jet', legend = True) # here, cmap = 'jet' specifies the colour ramp I've chosen to use.\n",
        "plt.autoscale(True)\n",
        "grid.plot(ax=ax, facecolor=\"none\", edgecolor='grey') # this is where you overlay the grid"
      ],
      "metadata": {
        "id": "gvxB3heoRB7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarising spatial patterns using a kernel density estimate\n",
        "\n",
        "We can also summarise the spatial distribution of points using a kernel density function. This is simple to achieve using the seaborn package. \n",
        "\n",
        "A kernel density function replaces plotting every single point by estimating the continuous observed probability distribution. It's not too dissimilar to the point densiyt counts that you've just carried out, but it differs in that it abstracts a surface that models the probability of point density over space. The idea behind kernel density estimates is to count the number of points in a continious way. Instead of using discrete counting, where you include a point in the count if it is inside a certain boundary and ignore it otherwise, KDEs use functions (kernels) that include points but give different weights to each one depending of how far of the location where we are counting the point is.\n",
        "\n",
        "To use the functions contained within the seaborn package, we need x and y coordinates of the points."
      ],
      "metadata": {
        "id": "Pj5N5hUCW_6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points[\"x\"] = points.geometry.x\n",
        "points[\"y\"] = points.geometry.y"
      ],
      "metadata": {
        "id": "4zhRtr_DXIZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have these x and y coordinates, creating a kernel density estimate is very straightfoward in Python. In its simplest form, we can run the following single line of code:"
      ],
      "metadata": {
        "id": "hszgac3TXn5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.kdeplot(data = points, x = \"x\", y = \"y\", shade=True, cmap='viridis', cbar = True)"
      ],
      "metadata": {
        "id": "vdfsvYbQXqEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much like with the bin size in the histogram, the ability of the kernel density estimate to accurately represent the data depends on the choice of smoothing bandwidth. An over-smoothed estimate might erase meaningful patterns, but an under-smoothed estimate can obscure the true patterns within random noise. The easiest way to check the robustness of the estimate is to adjust the default bandwidth (which is a backend calculation in seaborn using scipy, and is based on the distribution of the data).\n",
        "\n",
        "**Add a bw_adjust function (e.g. ```bw_adjust = 0.5```) to the code below, and have a play around to see how different values of bw_adjust alter the resulting kernel density estimation.**"
      ],
      "metadata": {
        "id": "B5ikPP-qdHYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seaborn.kdeplot(data = points, x = \"x\", y = \"y\", shade=True, cmap='viridis', cbar = True)"
      ],
      "metadata": {
        "id": "SPvavviRbvR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can read more about the different parameter options for kernel density estimates using the seaborn package [here](https://seaborn.pydata.org/generated/seaborn.kdeplot.html)."
      ],
      "metadata": {
        "id": "-wfwz4JKafcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2: Landslide area density\n",
        "To undertake density calculations, we need to divide the study area up into regions within which we are going to undertake density calculations.\n",
        "\n",
        "We can do this using the geopandas package.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gQwMB7EmV1Qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the grid over the data shows how the landslides are generally clustered into a few regions and we’ll end up with many “empty” grid cells:"
      ],
      "metadata": {
        "id": "eHzi-HEBY4oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1, figsize=(12, 8))\n",
        "LS.plot(ax=ax) # this is the landslide data\n",
        "plt.autoscale(True)\n",
        "grid.plot(ax=ax, facecolor=\"none\", edgecolor='grey') # this is where you overlay the grid!\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "nqzJIqV4Y-vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re going to determine the area of landslides in each square polygon. This takes a couple of steps in GeoPandas.\n",
        "\n",
        "First, we use the intersect tool to divide up the landslide polygons based on the grid cells. Then we calculate the area of each fragmented landslide polygon, and add the landslide index to the dataframe (just to make sure it gets carried over into the subsequent analysis)."
      ],
      "metadata": {
        "id": "cXVONUlmJMNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LS_intersection = LS.overlay(cell, keep_geom_type = False)\n",
        "LS_intersect = LS.overlay(grid, how='intersection', keep_geom_type=False)\n",
        "LS_intersect['LS_area'] =LS_intersect.apply(lambda row: row.geometry.area,axis=1) # calculate the area of each landslide segment\n",
        "LS_intersect['LS_index'] = LS_intersect.index\n",
        "print(LS_intersect)\n"
      ],
      "metadata": {
        "id": "zLToCM9Zij75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot out the data to check it looks sensible..."
      ],
      "metadata": {
        "id": "SgIUXbNWxL_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1, figsize=(25, 15))\n",
        "LS_intersect.plot(ax=ax, facecolor = \"none\", edgecolor = 'red')\n",
        "plt.autoscale(True)\n",
        "#cell.plot(ax=ax, facecolor=\"none\")#, edgecolor='grey') # this is where you overlay the grid"
      ],
      "metadata": {
        "id": "zYNhf4qYjFus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks good. You can clearly see where landslide polygons have been divided up where they cross grid cells."
      ],
      "metadata": {
        "id": "9ZjhIH17xXdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next task is to undertake a spatial join, to join the landslide data to the grid.We do this using the geopandas spatial join, ```sjoin``` tool.\n",
        "\n"
      ],
      "metadata": {
        "id": "xASaDDhTxev_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged = gpd.sjoin(grid, LS_intersect, predicate = 'contains', how='left')\n",
        "merged"
      ],
      "metadata": {
        "id": "qyIShIwWaaOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do you think that some rows in the dataframe have NaN values?**"
      ],
      "metadata": {
        "id": "N5w1q2_WxzDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we did before, we'll add the grid index to the dataframe:"
      ],
      "metadata": {
        "id": "_vozWSH5xtYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged['grid_index'] = merged.index # here, we add the index of the cells covering the study area into the dataframe so that we can use this in our analysis"
      ],
      "metadata": {
        "id": "7zJYxM5hntVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged"
      ],
      "metadata": {
        "id": "6SNhYEGCc6W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to sum up the area of landslides in each grid cell. In case there are more than one landslide per grid cell, we can dissolve the data, i.e. making the data more sparse, or in otherwords combine information (landslide area) from individual landslide polygons, to a total landslide area per grid cell."
      ],
      "metadata": {
        "id": "5-StOQvwkU4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute stats per grid cell -- aggregate landslides to grid cells with dissolve\n",
        "dissolve_merged = merged.dissolve(by='grid_index', aggfunc='sum') # sums up all the values in the area column\n",
        "dissolve_merged"
      ],
      "metadata": {
        "id": "-45r5Ycjkk7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, to help with landslide area density calculations, we need to total area of each grid cell. There are multiple ways we can calculate this (we specified the dimensions of each grid cell earlier). I always like to calculate dimensions based on the actual data, as there's less room for error."
      ],
      "metadata": {
        "id": "oTDxZ9bSy5CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dissolve_merged['cellarea'] =dissolve_merged.apply(lambda row: row.geometry.area,axis=1)\n",
        "dissolve_merged\n",
        "\n",
        "# Note that when you write out the dataframe, there's a little blue pen, which you can click to work interactivley with the dataframe, for instance if you want to interrogate the data manually."
      ],
      "metadata": {
        "id": "pA4TuPCwuDs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you've got the area of landsides in each grid cell you can finally calculate the landslide area density, which is simply the area of landslides in each cell, divided by the area of that cell, multiplied by 100, to give landslide area as a %.\n"
      ],
      "metadata": {
        "id": "12zBDNxb0HkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dissolve_merged['area_density'] = (dissolve_merged['LS_area']/dissolve_merged['cellarea'])*100\n",
        "dissolve_merged"
      ],
      "metadata": {
        "id": "rpSnB1bx1lk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, to plot the end product of this analysis:"
      ],
      "metadata": {
        "id": "Fvf81kSwzzAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1, figsize=(12, 8))\n",
        "dissolve_merged.plot(ax=ax, column='area_density', cmap='jet', legend = True) # here, cmap = 'jet' specifies the colour ramp I've chosen to use.\n",
        "plt.autoscale(True)\n",
        "grid.plot(ax=ax, facecolor=\"none\", edgecolor='grey') # this is where you overlay the grid\n"
      ],
      "metadata": {
        "id": "nsb0LoWik8I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Exercise 1 you calculated the point density of landslides and explored kernel density estimates. In Exercise 2 you calculated the area density of landslides. \n",
        "\n",
        "**Compare and contrast the resulting patterns of landslides from these different types of analyses. Have a think about which approach is most useful and why, given different hazards we're interested in, and the different data formats available to us (e.g. point/polygon/line) representations of hazards** \n",
        "\n"
      ],
      "metadata": {
        "id": "qtwHDAn2Ro05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extra task\n",
        "Have a play around with the grid size and evaluate effects of changing cell size on the resulting patterns of landslide area density."
      ],
      "metadata": {
        "id": "eN-2SufUiHUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Event effectiveness\n",
        "\n",
        "Linking back to the first part of the lecture on event effectiveness, to be able to determine effectiveness, we must be able to \n",
        "  1.\tconvert landslide area to some measure of effect, and\n",
        "  2.\tcalculate the effect of landslides in each size bin\n",
        "\n",
        "A reasonable measure of landslide ‘effect’ is the landslide volume – this indicates the amount of erodible sediment that was created by the landslide, and which may have hazardous impacts as it is mobilized and moved downstream. To determine this, we need to use an empirical scaling relationship to convert area to volume. For a sample of landslides that occurred following the Wenchuan earthquake, Parker et al (2011) found the relationship to be:\n",
        "\\begin{equation}\n",
        "    \\ V = 0.106 A^{1.388}\n",
        "\\end{equation}\n",
        "where *A* is area in m<sup>2</sup> and *V* is volume in m<sup>3</sup>.\n",
        "\n",
        "\n",
        "In a later study, [Xu et al (2016)](https://www.nature.com/articles/srep29797) found that:\n",
        "\\begin{equation}\n",
        "    \\ V = 0.135 A^{1.208}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "We can then apply this area-volume scaling relationship to each individual landslide, and then sum up the individual volumes.\n",
        "\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "SqMmm65BPkOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LS[\"volume\"] = 0.106*LS.area_rob**1.388\n"
      ],
      "metadata": {
        "id": "iMxNsCSwPck7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To divide the data into bins, you can use the following code to cut the data into bins, and then group the data into these different bins."
      ],
      "metadata": {
        "id": "6HgLS0U2f48J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = pd.cut(LS['area_rob'], bins=np.logspace(np.log10(min(LS.area_rob)),np.log10(max(LS.area_rob)), 30)) # The cut function converts continuous data into discrete bins.\n",
        "summaryLS = LS.groupby(g, observed=True).agg(area_count=('area_rob','count'), volume_sum=('volume','sum')).astype(float)\n",
        "summaryLS['index1'] = summaryLS.index\n",
        "\n"
      ],
      "metadata": {
        "id": "rAPfVho1sA4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaryLS"
      ],
      "metadata": {
        "id": "RY2JtzMIIRnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = np.logspace(np.log10(min(LS.area_rob)),np.log10(max(LS.area_rob)), 30)\n",
        "values = values[1:] # this line of code removes the first value form the array because for the sake of plotting the data, we'll take the upper limit of each bin interval\n",
        "print(values)"
      ],
      "metadata": {
        "id": "pyemm6ee_B0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volume_sum = np.array(summaryLS.volume_sum)\n",
        "#print(volume_sum)"
      ],
      "metadata": {
        "id": "DLBJnazGFdO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'volume_sum' is the effectiveness; i.e. the sum of individual landslide volumes in each bin.\n",
        "\n",
        "We can also plot out how landslide volume changes according to the equation above:\n"
      ],
      "metadata": {
        "id": "NDnUkKe5NjYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Volume = 0.106*values**1.388\n",
        "#print(Volume)"
      ],
      "metadata": {
        "id": "mH76ctn6Nips"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(values, summaryLS.area_count, color = 'red', label = 'frequency')\n",
        "ax.plot(values, Volume, color = 'green', label = 'volume')\n",
        "ax.plot(values, summaryLS.volume_sum, color = 'blue', label = 'effectiveness')\n",
        "ax.set_xlabel(\"Area m^2\")\n",
        "ax.set_ylabel(\"Frequency/Volume m^3/Effectiveness m^3\")\n",
        "ax.set_yscale('log')\n",
        "ax.set_xscale('log')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "RgGuKh6SxNep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do you observe about changes landslide effectiveness with an increase in landslide area?**\n",
        "**What are some limitations/uncertainties of this analysis?**"
      ],
      "metadata": {
        "id": "tvcAPpFZgW2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra task\n",
        "In the above task, we calculated landslide effectiveness using the Parker et al (2011) equation. How different is landslide effectiveness when calculated using the Xu et al (2016) area to volume relation? Differences in the results indicante a source of uncertainty in our estimatation of landslide effectiveness. \n",
        "What might other sources of uncertainty be?\n",
        "\n",
        "Linking back to the work you've undertaken during the previous few weeks, you could explore the frequency magnitude distribution of the landslides. Do they fit a powerlaw distribution in the same way that the earthquakes we've explored in the previous few weeks do?"
      ],
      "metadata": {
        "id": "kMKxAleCGptL"
      }
    }
  ]
}