{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXLIWfchuoBh3y5fsY89Tg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laura-turnbull-lloyd/STDH22_23/blob/main/Week_4_practical_1_STDH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical 1: Earthquake frequency-magnitude distributions, probabilities and recurrence intervals\n",
        "\n",
        "## Introduction\n",
        "This week you will undertake this self-led practical to explore the frequency-magnitude distributions of earthquakes in southern California, and the likelihood of a magnitude 6.7 and magnitude 8 earthquake occurring over the next 30 years.\n",
        "\n",
        "##Aims\n",
        "1. To acquire data from an earthquake catalogue\n",
        "2. To evaluate whether the  earthquake data are suitable for time-independent analyses\n",
        "3. To assess the frequency-magnitude relationship and undertake time-independent probability calculation\n",
        "\n",
        "##Format of the practical\n",
        "There are a set of tasks that you need to complete below (8 points possible). You also need to write a summary of your findings in a short scientific abstract of no more than 200 words (2 points possible). The abstract should provide a thorough summary of the work undertaken and should be well-written and organised. \n",
        "\n",
        "You will answer the questions and write your abstract in this notebook. \n",
        "\n",
        "##Submitting the practical\n",
        "You will submit this work in two forms:\n",
        "1. You are required to save this notebook as a pdf which you will upload to Turnitin. \n",
        "2. You are also required to submit this workbook as a notebook. To do this, please click on the 'share' button (top right corner of this screen). Under 'General Access' change the access from 'Restricted' to 'Anyone with link'. Then copy the link and paste it here:\n",
        "\n",
        "***Paste your link here:*** \n",
        "\n",
        "(note that to add text into a text block, you need to hover your mouse over the text block you want to edit, and double-click)\n",
        "\n",
        "Please also rename your workbook (Week_4_practical_1.ipynb) so that it contains your name (e.g. Week_4_practical_1_LauraTurnbullLloyd.ipynb).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QmBzWd80SuGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started\n",
        "First, you need to load modules."
      ],
      "metadata": {
        "id": "GWcc8Tk_iPzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import modules \n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from scipy import stats\n"
      ],
      "metadata": {
        "id": "CGSTtG0uiUUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading data\n",
        "Download data from the USGS using the following parameters:\n",
        "* format = 'csv'\n",
        "* starttime = 1994-01-01\n",
        "* endtime = 2008-01-01\n",
        "* minlatitude = 33.75\n",
        "* maxlatitude = 34.75\n",
        "* minlongitude = -119.20\n",
        "* maxlongitude = -117.80\n",
        "* minmagnitude = 1.0\n"
      ],
      "metadata": {
        "id": "1jN_nNFqYQnA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0E1vFrLSo_a"
      },
      "outputs": [],
      "source": [
        "# add required code here \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial exploration of the data\n",
        "As always, it is useful to plot the data to check it looks okay, and to check for any patterns. It is best to plot the earthquake time series with time on the x-axis and earthquake magnitude on the y-axis. \n",
        "\n",
        "Therefore, to improve plotting of the data, it is useful to convert the time column in the earthquake dataset into a time format recognised by python. You can then plot out the data and have better formatted axes labels showing dates. It also enables you to perform calculations on the date/time information, which you will shortly undertake.\n"
      ],
      "metadata": {
        "id": "yueqzNtLYe9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to convert the time column into a proper time format:\n",
        "eq.time = pd.to_datetime(eq.time)\n",
        "eq.sort_values(\"time\", inplace=True)\n"
      ],
      "metadata": {
        "id": "4NXDwmSWYn3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to plot the data\n",
        "fig = plt.figure()\n",
        "plt.plot(eq.time, eq.mag, \".\");\n",
        "plt.ylabel(\"Magnitude\");"
      ],
      "metadata": {
        "id": "YHBXFHnKc2OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Are large earthquakes clustered?\n",
        "The typical parameterization of earthquake periodicity or clustering is the Coefficient of Variation (CoV), which is the standard deviation of the interval between earthquakes divided by the mean recurrence interval, and is a straightforward metric for evaluating earthquake spacing where:\n",
        "* Periodic earthquake distributions have a COV < 1\n",
        "* Poissonian earthquake distribution have a COV = 1\n",
        "* Clustered earthquake distributions have a COV > 1\n"
      ],
      "metadata": {
        "id": "uPK_kjuwYqts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you're in a position to calculate the earthquake COV using the code below:"
      ],
      "metadata": {
        "id": "sws4mX37pVQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# earthquake COV analysis\n",
        "interarrival = eq.time.diff().dropna().apply(lambda x: x / np.timedelta64(1, \"D\")) # don't worry about the detail here. This code is basically calculating the time difference between earthquake dates and giving a result in days.\n",
        "\n",
        "eq_standarddeviation = np.std(interarrival)\n",
        "eq_mean = np.mean(interarrival)\n",
        "eq_cov = eq_standarddeviation/eq_mean\n",
        "print(eq_cov)\n"
      ],
      "metadata": {
        "id": "SkdvixoCY0MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1: What type of behaviour (clustered/Poissonian/periodic) is shown in the earthquake record? What does this type of behaviour mean in terms of time-independent probobability analyses? (1 point, 0.5 for each part of the question)\n",
        "\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "dGF56hPKvLXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cumulative frequency-magnitude distribution\n",
        "The next step is to inspect the frequency-magnitude distribution. Run the code below to plot out the frequency-magnitude distribution:"
      ],
      "metadata": {
        "id": "8dldo6TPY7kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show the cumulative frequency-magnitude distribution for the entire dataset\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(\n",
        "    eq[\"mag\"], bins=len(eq),\n",
        "    histtype=\"step\", log=True,\n",
        "    cumulative=-1)\n",
        "ax.set_xlabel(\"Magnitude\")\n",
        "ax.set_ylabel(\"Cumulative frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "42-3xeCeZAyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 2: Looking at the above plot, over what range do the data show an apparent powerlaw distribution? Why does this relation break down at the upper and lower extremes of the data? (1 point, 0.5 for each part of the question)\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "nu808nLkabtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rate of earthquakes\n",
        "You can use the earthquake inventory to determine the expected rate of earthquakes of a given magnitude or above per year. \n",
        "To calculate the expected rate, you need to count the number of earthquakes of a given magnitude or above in the dataset, add a new column to the dataframe (called 'rate'), and calculate the rate by dividing the count of earthquakes equal to or above a specific size by the number of years in the dataset. This will return the rate of earthquakes per year of a given magnitude or above.\n",
        "\n",
        "Recall that you counted the number earthquakes of a specific magnitude in week 2, as detailed below."
      ],
      "metadata": {
        "id": "wTCeGvfxZIsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#earthquake rate calculation\n",
        "from collections import Counter\n",
        "\n",
        "counted_magnitudes = Counter(eq[\"mag\"])\n",
        "magnitudes = set(eq[\"mag\"])\n",
        "magnitudes = sorted(list(magnitudes), reverse=True)\n",
        "frequency = np.zeros(len(magnitudes))\n",
        "frequency[0] = counted_magnitudes[magnitudes[0]]\n",
        "for i, magnitude in enumerate(magnitudes[1:]):\n",
        "    frequency[i + 1] = frequency[i] + counted_magnitudes[magnitude]"
      ],
      "metadata": {
        "id": "QFWb3_rhZ4I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you've got the overall frequency of earthquakes for each magnitude, you need to convert this to a rate per year:"
      ],
      "metadata": {
        "id": "9CoNWzyA12d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# answer\n",
        "#enter the calculation here to determine the rate, and save it in a variable called 'rate'\n"
      ],
      "metadata": {
        "id": "c4wXpBFY19zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the data\n",
        "fig, ax = plt.subplots()\n",
        "ax.semilogy(magnitudes, rate)\n",
        "ax.set_ylabel(\"rate per year N(M>=m)\")\n",
        "ax.set_xlabel(\"Magnitude\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-k7-WIru0Oct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fitting a powerlaw distribution to the data\n",
        "Recall that the relation between earthquake magnitude and frequency can be given by:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\log_{10}{R} = a - bM\n",
        "\\end{equation}\n",
        "\n",
        "where *M* is magnitude, *R* is the rate of events with magnitude >= *M*, and *a* and *b* are constants. Note that here were referring to rate, rather than number.\n",
        "\n",
        "What we are trying to do is find the best match between the predicted rate of earthquakes and the observed rate. Rearrange this equation to solve for R (rather than log(R), where R = rate)\n",
        "\n",
        "Recall that you need to optimise the fit of the powerlaw distribution by altering the range of earthquake magnitudes (xmin, xmax) over which you fit the powerlaw distribution. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wV64qGascPT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add values for xmin and xmax below, replacing '999' values\n",
        "mag_rate = pd.DataFrame({'mag': magnitudes, 'rate': rate}, columns=['mag', 'rate'])\n",
        "\n",
        "#subsample the dataframe based on a value of xmin and xmax\n",
        "#replace 999 with a suitable value\n",
        "mag_rate_subsample = mag_rate[(mag_rate['mag']>999) & (mag_rate['mag']<999)]\n"
      ],
      "metadata": {
        "id": "X_82l9k1r48m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now fit the model to the subsampled data in an attempt to optimise the model fit.\n",
        "Hint: this is where you fit the model using the np.polyfit function that you used in lecture 2"
      ],
      "metadata": {
        "id": "A4OZdN0F5KmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# answer\n"
      ],
      "metadata": {
        "id": "zWOhZoxk5a3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now visualize the model fit."
      ],
      "metadata": {
        "id": "iojjLxpj-VLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot your model (note the variable names might have to change here, depending on what you called them)\n",
        "fig, ax = plt.subplots()\n",
        "ax.semilogy(\n",
        "    magnitudes, rate, marker=\"+\", linestyle=\"None\",\n",
        "    label=\"Data\")\n",
        "ax.semilogy(mag_rate_subsample.mag, estimated_density_1, label=\"Model xmin = 'add number here', xmax = 'add number here'\")\n",
        "#ax.semilogy(magnitudes, estimated_density, label=\"Model\")\n",
        "ax.set_ylabel(\"Cumulative density\")\n",
        "ax.set_xlabel(\"Magnitude\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j2YmNMT15tLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 3. What is the best-fitting relationship that you find? Over what magnitude range does this best-fit relationship apply? (2 points; 1 point for the relationship, 1 point for the range)\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "H3Ivf5rMrr7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability and recurrence estimation\n",
        "Your next task is to calculate the time-independent probability and recurrence intervals of an earthquake of at least M 6.7 and M 8 occurring in this region over the next 30 years. Use the code block below to enter your calculations and answer Questions 4 and 5 below. \n",
        "\n",
        "Note that you will have to use the optimum a and b parameter values that you derived above to estimate the probability and recurrence intervals of larger earthquakes that are not contained within your dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gAnMCB35fIn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recurrence interval and probability calculations\n",
        "\n",
        "# Answer\n",
        "\n"
      ],
      "metadata": {
        "id": "wfhc_npRs8AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 4: What is the expected recurrence interval of an earthquake of at least M 6.6 and M 8? (1 point: 0.5 points for each part of the question)\n",
        "Double-click to enter your text here\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OVI7Du_gtCDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 5: What is the time-independent probability of an earthquake of at least M 6.7 and M 8 occurring in this region over the next 30 years? (1 point). How do these values compare with the findings of WGCEP for southern California? (1 point) (See https://pubs.usgs.gov/fs/2015/3009/pdf/fs2015-3009.pdf for summary)\n",
        "\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "TjTzVl_WlHir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 6: Briefly list the assumptions that we have made in estimating the probabilities and recurrence intervals in Questions 3 and 4. (1 point)\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "4XSdbinbk_Ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final task\n",
        "Summarise your findings in a brief (no more than 200 words) abstract. This should be a summary that is thorough and covers all the material (1 point), and that is well-written and well-organised (1 point)"
      ],
      "metadata": {
        "id": "jNU-ER1ftDN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract\n",
        "\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "T7Tz182OtUBr"
      }
    }
  ]
}