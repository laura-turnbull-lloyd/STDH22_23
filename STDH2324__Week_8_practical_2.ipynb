{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP14cltw38m38nrWObCSPm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laura-turnbull-lloyd/STDH_teaching/blob/main/STDH2324__Week_8_practical_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical 1: Earthquake frequency-magnitude distributions, probabilities and recurrence intervals\n",
        "\n",
        "## Introduction\n",
        "This week you will put ino practice what you have learned so far to undertake this self-led practical. You will explore the frequency-magnitude distributions of earthquakes in southern California, and the likelihood of a magnitude 6.7 and magnitude 8 earthquake occurring over the next 30 years.\n",
        "\n",
        "##Aims\n",
        "1. To acquire data from an earthquake catalogue\n",
        "2. To evaluate whether the  earthquake data are suitable for time-independent analyses\n",
        "3. To assess the frequency-magnitude relationship and undertake time-independent probability calculation\n",
        "\n",
        "##Format of the practical\n",
        "There are a set of tasks that you need to complete below. You also need to write a summary of your findings in a short scientific abstract of no more than 200 words. The abstract should provide a thorough summary of the work undertaken, assumptions of the work undertaken and should be well-written and organised.\n",
        "\n",
        "You will answer the questions and write your abstract in this notebook.\n",
        "\n",
        "##Submitting the practical\n",
        "You will submit this work in two forms:\n",
        "1. You are required to save this notebook as a pdf which you will upload to Turnitin.\n",
        "2. You are also required to submit this workbook as a notebook. To do this, please click on the 'share' button (top right corner of this screen). Under 'General Access' change the access from 'Restricted' to 'Anyone with link'. Then copy the link and paste it here:\n",
        "\n",
        "***Paste your link here:***\n",
        "\n",
        "(note that to add text into a text block, you need to hover your mouse over the text block you want to edit, and double-click)\n",
        "\n",
        "Please also rename your workbook (Week_8_practical_2.ipynb) so that it contains your name (e.g. Week_8_practical_2_LauraTurnbullLloyd.ipynb).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QmBzWd80SuGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started\n",
        "First, you need to load modules."
      ],
      "metadata": {
        "id": "GWcc8Tk_iPzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from scipy import stats\n"
      ],
      "metadata": {
        "id": "CGSTtG0uiUUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading data\n",
        "Download data from the USGS using the following parameters:\n",
        "* format = 'csv'\n",
        "* starttime = 1994-01-01\n",
        "* endtime = 2022-12-31\n",
        "* minlatitude = 33.75\n",
        "* maxlatitude = 34.75\n",
        "* minlongitude = -119.20\n",
        "* maxlongitude = -117.80\n",
        "* minmagnitude = 1.0\n",
        "\n",
        "Because there is a limit to how many records you can download in one go, first download data for 1994 to the end of 2007, and then 2008 to the end of 2022 (modifying the start and end times in the search parameters accordingly).\n",
        "\n",
        "Once you have read in the datasets (be sure to read them into different dataframe names, e.g. eq1 and eq2) you can then merge them using for example:\n",
        "\n",
        "frames = [eq1, eq2]\n",
        "\n",
        "all_eq = pd.concat(frames)\n"
      ],
      "metadata": {
        "id": "1jN_nNFqYQnA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0E1vFrLSo_a"
      },
      "outputs": [],
      "source": [
        "# add required code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial exploration of the data\n",
        "As always, it is useful to plot the data to check it looks okay, and to check for any patterns. It is best to plot the earthquake time series with time on the x-axis and earthquake magnitude on the y-axis.\n",
        "\n",
        "Therefore, to improve plotting of the data, it is useful to convert the time column in the earthquake dataset into a time format recognised by python. You can then plot out the data and have better formatted axes labels showing dates. It also enables you to perform calculations on the date/time information, which you will shortly undertake.\n"
      ],
      "metadata": {
        "id": "yueqzNtLYe9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to convert the time column into a proper time format:\n",
        "all_eq.time = pd.to_datetime(all_eq.time)\n",
        "all_eq.sort_values(\"time\", inplace=True)\n"
      ],
      "metadata": {
        "id": "4NXDwmSWYn3-"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to plot the data\n",
        "fig = plt.figure()\n",
        "plt.plot(all_eq.time, all_eq.mag, \".\");\n",
        "plt.ylabel(\"Magnitude\");"
      ],
      "metadata": {
        "id": "YHBXFHnKc2OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Are earthquakes clustered in space?\n",
        "As we've discussed before, it's always useful to view the spatial characteristics of data."
      ],
      "metadata": {
        "id": "K42A9z1gYHn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to create a map of earthquake locations and their magnitudes (as you did in week 1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "x = all_eq['longitude']\n",
        "y = all_eq['latitude']\n",
        "cbar = ax.scatter(x, y, c = all_eq.mag, alpha=0.8)\n",
        "ax.set_title('Title!')\n",
        "ax.set_xlabel('label me')\n",
        "ax.set_ylabel('me, too!')\n",
        "plt.colorbar(cbar, cmap='viridris', ax=ax, label='Earthquake magnitude', orientation='horizontal', pad = 0.1, shrink = 0.5)\n",
        "#ax.set_xlim(-160, -140)    # uncomment this and edit to zoom in\n",
        "#ax.set_ylim(15, 25)        # uncomment this and edit to zoom in\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WaQ2alegYQm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: Do the earthquakes appear to show any spatial clustering?\n",
        "\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "mReW1iCaYsKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Are earthquakes clustered in time?\n",
        "The typical parameterization of earthquake periodicity or clustering is the Coefficient of Variation (CV), which is the standard deviation of the interval between earthquakes divided by the mean recurrence interval, and is a straightforward metric for evaluating earthquake spacing where:\n",
        "* Periodic earthquake distributions have a CV < 1\n",
        "* Poissonian earthquake distribution have a CV = 1\n",
        "* Clustered earthquake distributions have a CV > 1\n"
      ],
      "metadata": {
        "id": "uPK_kjuwYqts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you're in a position to calculate the earthquake CV using the code below:"
      ],
      "metadata": {
        "id": "sws4mX37pVQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# earthquake CV analysis\n",
        "interarrival = all_eq.time.diff().dropna().apply(lambda x: x / np.timedelta64(1, \"D\")) # don't worry about the detail here. This code is basically calculating the time difference between earthquake dates and giving a result in days.\n",
        "\n",
        "eq_standarddeviation = np.std(interarrival)\n",
        "eq_mean = np.mean(interarrival)\n",
        "eq_cv = eq_standarddeviation/eq_mean\n",
        "print(eq_cv)\n"
      ],
      "metadata": {
        "id": "SkdvixoCY0MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: What type of behaviour (clustered/Poissonian/periodic) is shown in the earthquake record? What are the implications of this type of behaviour for time-independent probobability analyses?\n",
        "\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "dGF56hPKvLXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cumulative frequency-magnitude distribution\n",
        "The next step is to inspect the frequency-magnitude distribution. Run the code below to plot out the frequency-magnitude distribution:"
      ],
      "metadata": {
        "id": "8dldo6TPY7kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show the cumulative frequency-magnitude distribution for the entire dataset\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(\n",
        "    eq[\"mag\"], bins=len(all_eq),\n",
        "    histtype=\"step\", log=True,\n",
        "    cumulative=-1)\n",
        "ax.set_xlabel(\"Magnitude\")\n",
        "ax.set_ylabel(\"Cumulative frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "42-3xeCeZAyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 3: Looking at the above plot, over what range do the data show an apparent powerlaw distribution? Why does this relation break down at the upper and lower extremes of the data?\n",
        "\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "nu808nLkabtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rate of earthquakes\n",
        "You can use the earthquake inventory to determine the expected rate of earthquakes of a given magnitude or above per year.\n",
        "To calculate the expected rate, you need to count the number of earthquakes of a given magnitude or above in the dataset, add a new column to the dataframe (called 'rate'), and calculate the rate by dividing the count of earthquakes equal to or above a specific size by the number of years in the dataset. This will return the rate of earthquakes per year of a given magnitude or above. The code below does this for you.\n"
      ],
      "metadata": {
        "id": "wTCeGvfxZIsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#earthquake rate calculation\n",
        "from collections import Counter\n",
        "\n",
        "counted_magnitudes = Counter(all_eq[\"mag\"])\n",
        "magnitudes = set(all_eq[\"mag\"])\n",
        "magnitudes = sorted(list(magnitudes), reverse=True)\n",
        "frequency = np.zeros(len(magnitudes))\n",
        "frequency[0] = counted_magnitudes[magnitudes[0]]\n",
        "for i, magnitude in enumerate(magnitudes[1:]):\n",
        "    frequency[i + 1] = frequency[i] + counted_magnitudes[magnitude]"
      ],
      "metadata": {
        "id": "QFWb3_rhZ4I2"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you've got the overall frequency of earthquakes for each magnitude, you need to convert this to a rate per year:"
      ],
      "metadata": {
        "id": "9CoNWzyA12d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#enter the calculation here to determine the rate, and save it in a variable called 'rate'\n"
      ],
      "metadata": {
        "id": "c4wXpBFY19zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the data\n",
        "fig, ax = plt.subplots()\n",
        "ax.semilogy(magnitudes, rate)\n",
        "ax.set_ylabel(\"rate per year N(M>=m)\")\n",
        "ax.set_xlabel(\"Magnitude\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-k7-WIru0Oct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fitting a powerlaw distribution to the data\n",
        "Recall that the relation between earthquake magnitude and frequency can be given by:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\log_{10}{R} = a - bM\n",
        "\\end{equation}\n",
        "\n",
        "where *M* is magnitude, *R* is the rate of events with magnitude >= *M*, and *a* and *b* are constants. Note that here we're referring to rate, rather than number.\n",
        "\n",
        "What we are trying to do is find the best match between the predicted rate of earthquakes and the observed rate. Rearrange this equation to solve for R (rather than log(R), where R = rate)\n",
        "\n",
        "You will need to optimise the fit of the powerlaw distribution by altering the range of earthquake magnitudes (xmin, xmax) over which you fit the powerlaw distribution.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wV64qGascPT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, convert your magnitude and rate variables to a dataframe:\n",
        "mag_rate = pd.DataFrame({'mag': magnitudes, 'rate': rate}, columns=['mag', 'rate'])\n",
        "\n",
        "# Then, subsample the dataframe by adding values for xmin and xmax below, replacing '999' values\n",
        "\n",
        "mag_rate_subsample = mag_rate[(mag_rate['mag']>999) & (mag_rate['mag']<999)]\n"
      ],
      "metadata": {
        "id": "X_82l9k1r48m"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now fit the model to the subsampled data (i.e. the powerlaw fit between earthquake magnitude and rate of events) in an attempt to optimise the model fit, using the np.polyfit function (which you used in lecture 5). Remember that here you want to minimise the residual, whilst making use of as much of the earthquake inventory as possible..."
      ],
      "metadata": {
        "id": "A4OZdN0F5KmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using the np.polyfit function to fit the model to the data\n",
        "coefficients, residual, rank, singular_values, rcondition = np.polyfit(\n",
        "        mag_rate_subsample.mag, np.log10(mag_rate_subsample.rate), deg=1, full=True)\n",
        "b, a = coefficients\n",
        "b = abs(b) # we want the absolute value of b when fitting the model\n",
        "print(f\"b={b:.2f}\")\n",
        "print(f\"residual={residual}\")\n",
        "print(f\"a={10**a:.2f}\")"
      ],
      "metadata": {
        "id": "zWOhZoxk5a3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use your optimised a and b parameter values to estimate the cumulative rate of events by rearranging\n",
        "\n",
        "\\begin{equation}\n",
        "    \\log_{10}{R} = a - bM\n",
        "\\end{equation}\n",
        "\n",
        "to solve for R rather than log(R)."
      ],
      "metadata": {
        "id": "ZXt7hQC_G4L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enter your code here:\n",
        "\n"
      ],
      "metadata": {
        "id": "Wo2ZK087EEY3"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now visualize the model fit."
      ],
      "metadata": {
        "id": "iojjLxpj-VLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot your model (note that if you didn't assign the estimated cumulative rate of events to a variable called R, you'll need to rename R below)\n",
        "fig, ax = plt.subplots()\n",
        "ax.semilogy(\n",
        "    magnitudes, rate, marker=\"+\", linestyle=\"None\",\n",
        "    label=\"Data\")\n",
        "ax.semilogy(mag_rate_subsample.mag, R, label=\"Model xmin = 'add number here', xmax = 'add number here'\")\n",
        "#ax.semilogy(magnitudes, estimated_density, label=\"Model\")\n",
        "ax.set_ylabel(\"Cumulative density\")\n",
        "ax.set_xlabel(\"Magnitude\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j2YmNMT15tLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 4. What is the best-fitting relationship that you find? Over what magnitude range does this best-fit relationship apply?\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "H3Ivf5rMrr7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability and recurrence estimation\n",
        "Your next task is to calculate the time-independent probability and recurrence intervals of an earthquake of at least M 6.7 and M 8 occurring in this region over the next 30 years. Use the code block below to enter your calculations and answer Questions 4 and 5 below.\n",
        "\n",
        "You will have to use the optimum a and b parameter calues from your calculations above to estimate the probability and recurrence intervals of earthquakes this size, as they are larger than those contained within your earthquake.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gAnMCB35fIn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recurrence interval and probability calculations\n",
        "# Enter your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wfhc_npRs8AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 5: What is the expected recurrence interval of an earthquake of at least M 6.6 and M 8?\n",
        "Double-click to enter your text here\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OVI7Du_gtCDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 6: What is the time-independent probability of an earthquake of at least M 6.7 and M 8 occurring in this region over the next 30 years? How do these values compare with the findings of WGCEP for southern California? (See https://pubs.usgs.gov/fs/2015/3009/pdf/fs2015-3009.pdf for summary)\n",
        "\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "TjTzVl_WlHir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 7: Briefly list the assumptions that we have made in estimating the probabilities and recurrence intervals in Questions 5 and 6.\n",
        "\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "4XSdbinbk_Ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final task\n",
        "Summarise your findings in a brief (no more than 200 words) abstract. This should be a summary that is thorough and covers all the material, assumptions, and that is well-written and well-organised"
      ],
      "metadata": {
        "id": "jNU-ER1ftDN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract\n",
        "\n",
        "Double-click to enter your text here"
      ],
      "metadata": {
        "id": "T7Tz182OtUBr"
      }
    }
  ]
}